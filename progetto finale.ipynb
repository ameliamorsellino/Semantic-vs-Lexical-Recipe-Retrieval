{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ef04889",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "195f5b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/amelia/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/amelia/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/amelia/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/amelia/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/amelia/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from typing import List, Dict, Tuple, Optional, Union\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm \n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import nltk\n",
    "nltk.download([\"punkt\", \"stopwords\", \"wordnet\", \"averaged_perceptron_tagger\", \"punkt_tab\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ceb7d0f",
   "metadata": {},
   "source": [
    "### Test mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03f82ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test mode\n",
    "TEST_MODE = True \n",
    "SAMPLE_SIZE = 100000 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdaeb5fb",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47640046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recipes: 100,000 | Interactions: 488,928\n",
      "Recipes dataset: 100,000 recipes, 12 features\n",
      "Interactions dataset: 488,928 interactions, 5 features\n",
      "\n",
      "Recipe columns: ['name', 'id', 'minutes', 'contributor_id', 'submitted', 'tags', 'nutrition', 'n_steps', 'steps', 'description', 'ingredients', 'n_ingredients']\n",
      "\n",
      "Sample recipe entry:\n",
      "name                                      best ever chicken alfredo\n",
      "id                                                           131473\n",
      "minutes                                                          30\n",
      "contributor_id                                               204199\n",
      "submitted                                                2005-07-28\n",
      "tags              ['30-minutes-or-less', 'time-to-make', 'course...\n",
      "nutrition                [593.8, 28.0, 9.0, 42.0, 62.0, 22.0, 24.0]\n",
      "n_steps                                                           9\n",
      "steps             ['in large skillet , over medium-high heat , h...\n",
      "description       a quick and easy pasta dish made with mushroom...\n",
      "ingredients       ['olive oil', 'garlic cloves', 'boneless skinl...\n",
      "n_ingredients                                                    12\n",
      "Name: 22440, dtype: object\n",
      "Merged dataset size: 488,928 rows, 16 columns\n",
      "\n",
      "Columns with missing values:\n",
      "  description: 10,405 (2.13%)\n",
      "  review: 71 (0.01%)\n",
      "Nutritional features added:\n",
      "        calories  total_fat      sugar     sodium    protein  saturated_fat  \\\n",
      "count  455870.00  455870.00  455870.00  455870.00  455870.00      455870.00   \n",
      "mean      346.24      25.83      48.60      22.67      29.87          32.78   \n",
      "std       241.89      24.80      72.62      24.63      30.79          35.10   \n",
      "min         0.00       0.00       0.00       0.00       0.00           0.00   \n",
      "25%       166.10       8.00       8.00       6.00       6.00           7.00   \n",
      "50%       293.30      19.00      21.00      15.00      17.00          21.00   \n",
      "75%       468.40      36.00      57.00      31.00      48.00          47.00   \n",
      "max      2957.60     193.00     664.00     180.00     159.00         193.00   \n",
      "\n",
      "       carbohydrates  \n",
      "count      455870.00  \n",
      "mean           10.61  \n",
      "std             9.74  \n",
      "min             0.00  \n",
      "25%             4.00  \n",
      "50%             8.00  \n",
      "75%            15.00  \n",
      "max            70.00  \n",
      "Unique recipes in dataset: 93,410\n",
      "\n",
      "Top 20 most common ingredients:\n",
      "       ingredient  count\n",
      "             salt  34060\n",
      "           butter  21713\n",
      "            sugar  17044\n",
      "            onion  16243\n",
      "            water  13871\n",
      "        olive oil  13723\n",
      "             eggs  13252\n",
      "    garlic cloves  10576\n",
      "             milk  10386\n",
      "            flour  10181\n",
      "           pepper   9182\n",
      "      brown sugar   7480\n",
      "           garlic   7347\n",
      "              egg   7001\n",
      "    baking powder   6841\n",
      "all-purpose flour   6825\n",
      "  salt and pepper   6370\n",
      "  parmesan cheese   6167\n",
      "      lemon juice   5763\n",
      "    vegetable oil   5475\n",
      "\n",
      "Top 30 most common tags:\n",
      "               tag  frequency\n",
      "       preparation      93010\n",
      "      time-to-make      90880\n",
      "            course      87951\n",
      "   main-ingredient      69556\n",
      "           dietary      66336\n",
      "              easy      51269\n",
      "          occasion      45981\n",
      "           cuisine      36822\n",
      "  low-in-something      34375\n",
      "         main-dish      29265\n",
      "60-minutes-or-less      28817\n",
      "         equipment      28315\n",
      "30-minutes-or-less      22857\n",
      "number-of-servings      22773\n",
      "              meat      22741\n",
      "        vegetables      22576\n",
      "        taste-mood      21152\n",
      "    north-american      19619\n",
      "   4-hours-or-less      19029\n",
      "   3-steps-or-less      18215\n",
      "15-minutes-or-less      17692\n",
      "        low-sodium      17551\n",
      "          low-carb      17042\n",
      "          desserts      16836\n",
      "           healthy      16245\n",
      "       low-calorie      15367\n",
      "      dinner-party      15149\n",
      "   low-cholesterol      14899\n",
      "        vegetarian      14741\n",
      "     beginner-cook      14725\n",
      "\n",
      "Semantic tags present in dataset:\n",
      "comfort-food: 10,567 recipes\n",
      "healthy: 16,245 recipes\n",
      "easy: 51,269 recipes\n",
      "romantic: 2,060 recipes\n",
      "vegetarian: 14,741 recipes\n",
      "low-carb: 17,042 recipes\n",
      "lunch: 9,893 recipes\n",
      "breakfast: 5,718 recipes\n",
      "italian: 3,092 recipes\n",
      "mexican: 2,763 recipes\n",
      "asian: 5,420 recipes\n",
      "summer: 4,128 recipes\n",
      "winter: 3,148 recipes\n",
      "  Cleaned: name\n",
      "  Cleaned: tags\n",
      "  Cleaned: description\n",
      "  Cleaned: ingredients\n",
      "  Cleaned: steps\n",
      "\n",
      "Removed 0 recipes with insuffcient text\n",
      "Final corpus size: 93,410 recipes\n",
      "  'comfort food': 11,104 potential matches\n",
      "  'healthy dinner': 3,412 potential matches\n",
      "  'quick breakfast': 1,391 potential matches\n",
      "  'romantic dinner': 1,435 potential matches\n",
      "  'vegetarian lunch': 2,390 potential matches\n",
      "  'low carb': 17,844 potential matches\n",
      "  'summer dessert': 1,151 potential matches\n",
      "  'holiday cookies': 1,670 potential matches\n",
      "Loaded 93,410 recipes\n",
      "Average document length: 175 words\n"
     ]
    }
   ],
   "source": [
    "# DATA LOADING\n",
    "recipes_df = pd.read_csv(\"RAW_recipes.csv\")\n",
    "interactions_df = pd.read_csv(\"RAW_interactions.csv\")\n",
    "if TEST_MODE:\n",
    "    recipes_df = recipes_df.sample(n=min(SAMPLE_SIZE, len(recipes_df)), random_state=5)\n",
    "    interactions_df = interactions_df[interactions_df['recipe_id'].isin(recipes_df['id'])]\n",
    "print(f\"Recipes: {len(recipes_df):,} | Interactions: {len(interactions_df):,}\")\n",
    "\n",
    "\n",
    "print(f\"Recipes dataset: {recipes_df.shape[0]:,} recipes, {recipes_df.shape[1]} features\")\n",
    "print(f\"Interactions dataset: {interactions_df.shape[0]:,} interactions, {interactions_df.shape[1]} features\")\n",
    "\n",
    "# Preview recipe structure\n",
    "print(\"\\nRecipe columns:\", list(recipes_df.columns))\n",
    "print(\"\\nSample recipe entry:\")\n",
    "print(recipes_df.iloc[0])\n",
    "\n",
    "\n",
    "# DATA MERGING\n",
    "# Join recipes with user interactions\n",
    "merged_data = recipes_df.merge(interactions_df, how=\"inner\", left_on=\"id\", right_on=\"recipe_id\")\n",
    "\n",
    "# Remove duplicate column after merge\n",
    "if \"recipe_id\" in merged_data.columns:\n",
    "    merged_data = merged_data.drop(columns=[\"recipe_id\"])\n",
    "\n",
    "print(f\"Merged dataset size: {merged_data.shape[0]:,} rows, {merged_data.shape[1]} columns\")\n",
    "\n",
    "# Check for missing data\n",
    "null_summary = merged_data.isnull().sum()\n",
    "null_present = null_summary[null_summary > 0]\n",
    "print(\"\\nColumns with missing values:\")\n",
    "if len(null_present) > 0:\n",
    "    for col, count in null_present.items():\n",
    "        pct = (count / len(merged_data)) * 100\n",
    "        print(f\"  {col}: {count:,} ({pct:.2f}%)\")\n",
    "else:\n",
    "    print(\"none found\")\n",
    "\n",
    "\n",
    "# NUTRITIONAL DATA EXTRACTION\n",
    "nutrition_columns = [\"calories\", \"total_fat\", \"sugar\", \"sodium\", \"protein\", \"saturated_fat\", \"carbohydrates\"]\n",
    "\n",
    "def parse_nutrition_string(nutrition_str: str) -> List[float]:\n",
    "    try:\n",
    "        cleaned = str(nutrition_str).strip(\"[]\")\n",
    "        values = [float(v.strip()) for v in cleaned.split(\",\")]\n",
    "        if len(values) != 7:\n",
    "            return [np.nan] * 7\n",
    "        return values\n",
    "    except (ValueError, AttributeError):\n",
    "        return [np.nan] * 7\n",
    "\n",
    "# Apply parsing and create new columns\n",
    "nutrition_values = merged_data[\"nutrition\"].apply(parse_nutrition_string)\n",
    "nutrition_df = pd.DataFrame(nutrition_values.tolist(), columns=nutrition_columns, index=merged_data.index)\n",
    "\n",
    "# Merge with dataframe\n",
    "merged_data = pd.concat([merged_data, nutrition_df], axis=1)\n",
    "\n",
    "for col in nutrition_columns:\n",
    "    upper = merged_data[col].quantile(0.99)\n",
    "    merged_data = merged_data[merged_data[col] <= upper]\n",
    "\n",
    "print(\"Nutritional features added:\")\n",
    "print(merged_data[nutrition_columns].describe().round(2)) # statistical info\n",
    "\n",
    "\n",
    "# RECIPE CHARACTERISTICS ANALYSIS\n",
    "# Get unique recipes for analysis\n",
    "unique_recipes = merged_data.drop_duplicates(subset=[\"id\"])\n",
    "print(f\"Unique recipes in dataset: {len(unique_recipes):,}\")\n",
    "\n",
    "\n",
    "# INGREDIENT ANALYSIS\n",
    "def extract_ingredients_list(ing_string: str) -> List[str]:\n",
    "    try:\n",
    "        cleaned = str(ing_string).replace(\"[\", \"\").replace(\"]\", \"\").replace(\"'\", \"\")\n",
    "        ingredients = [i.strip().lower() for i in cleaned.split(\",\") if i.strip()]\n",
    "        return ingredients\n",
    "    except (AttributeError, TypeError):\n",
    "        return []\n",
    "\n",
    "# Extract all ingredients\n",
    "all_ingredients = []\n",
    "for ing_str in unique_recipes[\"ingredients\"]:\n",
    "    all_ingredients.extend(extract_ingredients_list(ing_str))\n",
    "\n",
    "# Count frequency\n",
    "ingredient_counts = Counter(all_ingredients)\n",
    "top_ingredients = pd.DataFrame(ingredient_counts.most_common(20), columns=[\"ingredient\", \"count\"])\n",
    "\n",
    "print(\"\\nTop 20 most common ingredients:\")\n",
    "print(top_ingredients.to_string(index=False))\n",
    "\n",
    "\n",
    "# TAG ANALYSIS\n",
    "def extract_tags_list(tag_string: str) -> List[str]:\n",
    "    try:\n",
    "        cleaned = str(tag_string).replace(\"[\", \"\").replace(\"]\", \"\").replace(\"'\", \"\")\n",
    "        tags = [t.strip().lower() for t in cleaned.split(\",\") if t.strip()]\n",
    "        return tags\n",
    "    except (AttributeError, TypeError):\n",
    "        return []\n",
    "\n",
    "# Extract all tags\n",
    "all_tags = []\n",
    "for tag_str in unique_recipes[\"tags\"]:\n",
    "    all_tags.extend(extract_tags_list(tag_str))\n",
    "\n",
    "# Count frequency\n",
    "tag_counts = Counter(all_tags)\n",
    "top_tags = pd.DataFrame(tag_counts.most_common(30), columns=[\"tag\", \"frequency\"])\n",
    "\n",
    "print(\"\\nTop 30 most common tags:\")\n",
    "print(top_tags.to_string(index=False))\n",
    "\n",
    "# Semantic tags for analysis\n",
    "semantic_tags = [\"comfort-food\", \"healthy\", \"quick\", \"easy\", \"romantic\", \"vegetarian\", \n",
    "                 \"low-carb\", \"dessert\", \"dinner\", \"lunch\", \"breakfast\", \"italian\",\n",
    "                 \"mexican\", \"asian\", \"mediterranean\", \"summer\", \"winter\", \"holiday\"]\n",
    "\n",
    "print(\"\\nSemantic tags present in dataset:\")\n",
    "for tag in semantic_tags:\n",
    "    count = tag_counts.get(tag, 0)\n",
    "    if count > 0:\n",
    "        print(f\"{tag}: {count:,} recipes\")\n",
    "\n",
    "\n",
    "# CORPUS CREATION\n",
    "# Create corpus dataframe with unique recipes\n",
    "corpus = unique_recipes[[\"id\", \"name\", \"tags\", \"description\", \"ingredients\", \"steps\"]].copy()\n",
    "\n",
    "# Clean each text field\n",
    "text_columns = [\"name\", \"tags\", \"description\", \"ingredients\", \"steps\"]\n",
    "\n",
    "for col in text_columns:\n",
    "    corpus[col + \"_clean\"] = (\n",
    "        corpus[col]\n",
    "        .fillna(\"\") # removing NaN\n",
    "        .astype(str)\n",
    "        .str.replace(\"[\", \"\", regex=False)\n",
    "        .str.replace(\"]\", \"\", regex=False)\n",
    "        .str.replace(\"'\", \"\", regex=False)\n",
    "        .str.replace('\"', \"\", regex=False)\n",
    "        .str.lower()\n",
    "        .str.replace(r\"\\s+\", \" \", regex=True)\n",
    "        .str.strip()\n",
    "    )\n",
    "    print(f\"  Cleaned: {col}\")\n",
    "\n",
    "# Combine all text into single document per recipe\n",
    "corpus[\"document\"] = (\n",
    "    corpus[\"name_clean\"] + \" \" +\n",
    "    corpus[\"tags_clean\"] + \" \" +\n",
    "    corpus[\"description_clean\"] + \" \" +\n",
    "    corpus[\"ingredients_clean\"] + \" \" +\n",
    "    corpus[\"steps_clean\"]\n",
    ")\n",
    "\n",
    "# Final cleaning\n",
    "corpus[\"document\"] = (\n",
    "    corpus[\"document\"]\n",
    "    .str.replace(r\"\\s+\", \" \", regex=True)\n",
    "    .str.strip()\n",
    ")\n",
    "\n",
    "# Add word count\n",
    "corpus[\"word_count\"] = corpus[\"document\"].str.split().str.len()\n",
    "\n",
    "# Remove recipes with empty documents\n",
    "initial_count = len(corpus)\n",
    "corpus = corpus[corpus[\"document\"].str.len() > 10].copy()\n",
    "print(f\"\\nRemoved {initial_count - len(corpus)} recipes with insuffcient text\")\n",
    "print(f\"Final corpus size: {len(corpus):,} recipes\")\n",
    "\n",
    "\n",
    "# EXPORT CORPUS\n",
    "# Save full corpus with metadata\n",
    "corpus_export = corpus[[\"id\", \"name\", \"tags_clean\", \"document\", \"word_count\"]].copy()\n",
    "corpus_export.columns = [\"recipe_id\", \"recipe_name\", \"tags\", \"document\", \"word_count\"]\n",
    "corpus_export.to_csv(\"search_corpus.csv\", index=False)\n",
    "\n",
    "# Save recipe metadata\n",
    "metadata = unique_recipes[[\"id\", \"name\", \"minutes\", \"n_ingredients\", \"n_steps\", \"description\"]].copy()\n",
    "metadata.columns = [\"recipe_id\", \"recipe_name\", \"cooking_time\", \"num_ingredients\", \"num_steps\", \"description\"]\n",
    "metadata.to_csv(\"recipe_metadata.csv\", index=False)\n",
    "\n",
    "\n",
    "# CORPUS QUALITY CHECK\n",
    "test_queries = [\n",
    "    \"comfort food\",\n",
    "    \"healthy dinner\",\n",
    "    \"quick breakfast\",\n",
    "    \"romantic dinner\",\n",
    "    \"vegetarian lunch\",\n",
    "    \"low carb\",\n",
    "    \"summer dessert\",\n",
    "    \"holiday cookies\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    query_terms = query.lower().split()\n",
    "    mask = pd.Series([True] * len(corpus), index=corpus.index)\n",
    "    for term in query_terms:\n",
    "        mask = mask & corpus[\"document\"].str.contains(term, regex=False)\n",
    "    matches = mask.sum()\n",
    "    print(f\"  '{query}': {matches:,} potential matches\")\n",
    "\n",
    "\n",
    "# LOAD CORPUS FOR SEARCH ENGINES\n",
    "corpus_df = pd.read_csv(\"search_corpus.csv\")\n",
    "metadata_df = pd.read_csv(\"recipe_metadata.csv\")\n",
    "\n",
    "print(f\"Loaded {len(corpus_df):,} recipes\")\n",
    "print(f\"Average document length: {corpus_df['word_count'].mean():.0f} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2ba8ae",
   "metadata": {},
   "source": [
    "### TF-IDF search engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537ef155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fiting TF-IDF model\n",
      "Preprocessing documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing:   0%|          | 401/93410 [00:00<01:09, 1335.33it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing: 100%|██████████| 93410/93410 [01:07<00:00, 1383.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing TF-IDF matrix\n",
      "\n",
      "TF-IDF Model Statistics:\n",
      "Documents: 93,410\n",
      "Vocabulary size: 50,000\n",
      "N-gram range: (1, 2)\n",
      "Matrix shape: (93410, 50000)\n",
      "Matrix sparsity: 99.72%\n",
      "Non-zero elements: 13,038,065\n",
      "\n",
      "--- Query Analysis for: 'chocolate cake' ---\n",
      "Processed query: 'chocolate cake'\n",
      "Matched unigrams: ['chocolate', 'cake']\n",
      "Matched n-grams: ['chocolate cake']\n",
      "Unmatched terms: []\n",
      "\n",
      "[1] store bought chocolate cake and milk\n",
      "Score: 0.3790\n",
      "Cooking Time: 5 min | Ingredients: 2 | Steps: 4\n",
      "Description: this is a recipe that my friend's dad would always make when we were in elementary school. they bought a chocolate cake with vanilla icing. sliced it ...\n",
      "\n",
      "[2] cake mix chocolate cookies\n",
      "Score: 0.3639\n",
      "Cooking Time: 35 min | Ingredients: 5 | Steps: 4\n",
      "Description: here is a real easy one.  just use a chocolate cake mix.  i add a hand full of chocolate chips to get a double chocolaute fix....\n",
      "\n",
      "[3] miracle whip chocolate cake\n",
      "Score: 0.3487\n",
      "Cooking Time: 55 min | Ingredients: 8 | Steps: 3\n",
      "Description: this is a very easy yet absolutely scrumptious and moist chocolate cake. the recipe was lost when my mom died but was found years later in a church co...\n",
      "\n",
      "--- Query Analysis for: '' ---\n",
      "Processed query: ''\n",
      "Matched unigrams: []\n",
      "Matched n-grams: []\n",
      "Unmatched terms: []\n",
      "No results found.\n",
      "\n",
      "--- Query Analysis for: 'chicken soup' ---\n",
      "Processed query: 'chicken soup'\n",
      "Matched unigrams: ['chicken', 'soup']\n",
      "Matched n-grams: ['chicken soup']\n",
      "Unmatched terms: []\n",
      "\n",
      "[1] my own lazy day recipe   chicken noodle soup\n",
      "Score: 0.2925\n",
      "Cooking Time: 35 min | Ingredients: 6 | Steps: 4\n",
      "Description: i made this up one day when i wanted some old-fashioned tasting chicken soup without all the bother and i love it....\n",
      "\n",
      "[2] super simple and speedy chicken soup\n",
      "Score: 0.2876\n",
      "Cooking Time: 15 min | Ingredients: 6 | Steps: 5\n",
      "Description: need chicken soup fast and don't have time to cook that bird for hours? this has happened to us plenty of times, when someone has come home in the moo...\n",
      "\n",
      "[3] eggs   chicken soup\n",
      "Score: 0.2853\n",
      "Cooking Time: 7 min | Ingredients: 2 | Steps: 5\n",
      "Description: an excellent twist on the usual breakfast foods.  very tasty!...\n",
      "\n",
      "--- Query Analysis for: 'comfort food for a rainy day' ---\n",
      "Processed query: 'comfort food rainy day'\n",
      "Matched unigrams: ['comfort', 'food', 'rainy', 'day']\n",
      "Matched n-grams: ['comfort food', 'rainy day']\n",
      "Unmatched terms: []\n",
      "\n",
      "[1] janet s banana bread\n",
      "Score: 0.2388\n",
      "Cooking Time: 50 min | Ingredients: 12 | Steps: 6\n",
      "Description: is a comfort food on a rainy day, you can tailor the recipe your own way! simple and easy to do, i hope you'll take the time and enjoy it too!...\n",
      "\n",
      "[2] no bake butterscotch drops\n",
      "Score: 0.2335\n",
      "Cooking Time: 15 min | Ingredients: 3 | Steps: 4\n",
      "Description: these are cindy's and the cornflakes give it a different flavor. fun for kids to make on a rainy day. good for them too...\n",
      "\n",
      "[3] rainy day yellow cake\n",
      "Score: 0.2236\n",
      "Cooking Time: 50 min | Ingredients: 10 | Steps: 26\n",
      "Description: first made on 12/27/02. best cake ever! always use for birthday cakes. (double recipe for my large cake pan)....\n"
     ]
    }
   ],
   "source": [
    "# TEXT PREPROCESSING CLASS\n",
    "class TextPreprocessor:\n",
    "    def __init__(self, \n",
    "                 remove_stopwords: bool = True, \n",
    "                 use_lemmatization: bool = True,\n",
    "                 use_stemming: bool = False,\n",
    "                 min_word_length: int = 2,\n",
    "                 custom_stopwords: Optional[set] = None):\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        self.use_lemmatization = use_lemmatization\n",
    "        self.use_stemming = use_stemming\n",
    "        self.min_word_length = min_word_length\n",
    "        \n",
    "        # Initialize NLTK tools\n",
    "        self.lemmatizer = WordNetLemmatizer() if use_lemmatization else None\n",
    "        self.stemmer = PorterStemmer() if use_stemming else None\n",
    "        \n",
    "        # Build stopword set\n",
    "        self.stopwords = set(stopwords.words('english')) if remove_stopwords else set()\n",
    "        \n",
    "        # Add recipe-specific stopwords\n",
    "        recipe_stopwords = {\n",
    "            'cup', 'cups', 'tablespoon', 'tablespoons', 'teaspoon', 'teaspoons',\n",
    "            'tbsp', 'tsp', 'oz', 'ounce', 'ounces', 'pound', 'pounds', 'lb', 'lbs',\n",
    "            'inch', 'inches', 'minute', 'minutes', 'hour', 'hours',\n",
    "            'medium', 'large', 'small', 'fresh', 'chopped', 'minced', 'diced',\n",
    "            'add', 'place', 'put', 'make', 'use', 'take', 'get', 'set',\n",
    "            'recipe', 'recipes', 'ingredient', 'ingredients', 'step', 'steps',\n",
    "            'one', 'two', 'three', 'four', 'five', 'six', 'time', 'preparation',\n",
    "            'optional', 'needed', 'taste', 'degree', 'degrees'\n",
    "        }\n",
    "        self.stopwords.update(recipe_stopwords)\n",
    "        \n",
    "        if custom_stopwords:\n",
    "            self.stopwords.update(custom_stopwords)\n",
    "    \n",
    "    def preprocess(self, text: str) -> str:\n",
    "        \n",
    "        if pd.isna(text) or not isinstance(text, str) or not text.strip():\n",
    "            return \"\"\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "        \n",
    "        # Tokenize\n",
    "        try:\n",
    "            tokens = word_tokenize(text)\n",
    "        except Exception:\n",
    "            tokens = text.split()\n",
    "        \n",
    "        # Process tokens\n",
    "        processed_tokens = []\n",
    "        for token in tokens:\n",
    "            if len(token) < self.min_word_length:\n",
    "                continue\n",
    "            \n",
    "            if self.remove_stopwords and token in self.stopwords:\n",
    "                continue\n",
    "            \n",
    "            if self.use_lemmatization and self.lemmatizer:\n",
    "                token = self.lemmatizer.lemmatize(token, pos='v')\n",
    "                token = self.lemmatizer.lemmatize(token, pos='n')\n",
    "            elif self.use_stemming and self.stemmer:\n",
    "                token = self.stemmer.stem(token)\n",
    "            \n",
    "            processed_tokens.append(token)\n",
    "        return ' '.join(processed_tokens)\n",
    "    \n",
    "    def preprocess_batch(self, texts: Union[List[str], pd.Series], show_progress: bool = True) -> List[str]:\n",
    "        \n",
    "        if isinstance(texts, pd.Series): # convert to list so that len() works\n",
    "            texts = texts.tolist()\n",
    "        else:\n",
    "            texts = list(texts)\n",
    "        processed = []\n",
    "        \n",
    "        iterator = tqdm(texts, desc=\"Preprocessing\", disable=not show_progress)\n",
    "        for text in iterator:\n",
    "            processed.append(self.preprocess(text))\n",
    "        return processed\n",
    "\n",
    "# TF-IDF SEARCH ENGINE CLASS\n",
    "class TFIDFSearchEngine: \n",
    "    def __init__(self, \n",
    "                 ngram_range: Tuple[int, int] = (1, 2),\n",
    "                 max_features: int = 50000, # limit for memory reasons\n",
    "                 min_df: int = 2,\n",
    "                 max_df: float = 0.95,\n",
    "                 sublinear_tf: bool = True): # 1 + log(tf)\n",
    "        self.ngram_range = ngram_range\n",
    "        self.max_features = max_features\n",
    "        \n",
    "        self.vectorizer = TfidfVectorizer( # Converts a collection of raw documents to a matrix of TF-IDF features\n",
    "            ngram_range=ngram_range,\n",
    "            max_features=max_features,\n",
    "            min_df=min_df,\n",
    "            max_df=max_df,\n",
    "            sublinear_tf=sublinear_tf,\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        self.preprocessor = TextPreprocessor(\n",
    "            remove_stopwords=True,\n",
    "            use_lemmatization=True,\n",
    "            use_stemming=False\n",
    "        )\n",
    "        \n",
    "        self.tfidf_matrix = None\n",
    "        self.document_ids = None\n",
    "        self.id_to_index: Dict[int, int] = {}  \n",
    "        self.is_fitted = False\n",
    "        \n",
    "    def fit(self, documents: Union[List[str], pd.Series], \n",
    "            document_ids: Optional[List] = None, \n",
    "            preprocess: bool = True) -> None:\n",
    "        \n",
    "        print(\"Fiting TF-IDF model\")\n",
    "        \n",
    "        # Convert to list\n",
    "        if isinstance(documents, pd.Series):\n",
    "            documents = documents.tolist()\n",
    "        else:\n",
    "            documents = list(documents)\n",
    "        \n",
    "        # Store document IDs\n",
    "        if document_ids is not None:\n",
    "            self.document_ids = list(document_ids)\n",
    "        else:\n",
    "            self.document_ids = list(range(len(documents)))\n",
    "        \n",
    "        # Build fast lookup dictionary\n",
    "        self.id_to_index = {doc_id: idx for idx, doc_id in enumerate(self.document_ids)}\n",
    "        \n",
    "        # Preprocess documents\n",
    "        if preprocess:\n",
    "            print(\"Preprocessing documents\")\n",
    "            processed_docs = self.preprocessor.preprocess_batch(documents, show_progress=True)\n",
    "        else:\n",
    "            processed_docs = documents\n",
    "        \n",
    "        # Fit and transform TF-IDF\n",
    "        print(\"Computing TF-IDF matrix\")\n",
    "        self.tfidf_matrix = self.vectorizer.fit_transform(processed_docs)\n",
    "        self.is_fitted = True\n",
    "        \n",
    "        # statistics\n",
    "        vocab_size = len(self.vectorizer.vocabulary_)\n",
    "        n_docs = self.tfidf_matrix.shape[0]\n",
    "        sparsity = 1.0 - (self.tfidf_matrix.nnz / (n_docs * vocab_size)) \n",
    "        \n",
    "        print(f\"\\nTF-IDF Model Statistics:\")\n",
    "        print(f\"Documents: {n_docs:,}\")\n",
    "        print(f\"Vocabulary size: {vocab_size:,}\")\n",
    "        print(f\"N-gram range: {self.ngram_range}\")\n",
    "        print(f\"Matrix shape: {self.tfidf_matrix.shape}\")\n",
    "        print(f\"Matrix sparsity: {sparsity:.2%}\")\n",
    "        print(f\"Non-zero elements: {self.tfidf_matrix.nnz:,}\")\n",
    "        \n",
    "    def search(self, query: str, top_k: int = 10, preprocess: bool = True) -> List[Tuple[int, float]]:\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Model not fitted fit() first\")\n",
    "        \n",
    "        # Validate query\n",
    "        if not query or not query.strip():\n",
    "            return []\n",
    "        \n",
    "        # Preprocess query\n",
    "        if preprocess:\n",
    "            processed_query = self.preprocessor.preprocess(query)\n",
    "        else:\n",
    "            processed_query = query\n",
    "        \n",
    "        # Check if query has any valid terms after preprocessing\n",
    "        if not processed_query.strip():\n",
    "            return []\n",
    "        \n",
    "        # Transform query to TF-IDF vector\n",
    "        query_vector = self.vectorizer.transform([processed_query])\n",
    "        \n",
    "        # Compute cosine similarities\n",
    "        similarities = cosine_similarity(query_vector, self.tfidf_matrix).flatten()\n",
    "        \n",
    "        # Get top-k results\n",
    "        top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "        \n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            doc_id = self.document_ids[idx]\n",
    "            score = float(similarities[idx])\n",
    "            if score > 0:  # Only include results with positive similarity\n",
    "                results.append((doc_id, score))\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_query_terms(self, query: str, preprocess: bool = True) -> Dict:\n",
    "        if preprocess:\n",
    "            processed_query = self.preprocessor.preprocess(query)\n",
    "        else:\n",
    "            processed_query = query\n",
    "        \n",
    "        query_terms = processed_query.split()\n",
    "        vocabulary = set(self.vectorizer.vocabulary_.keys())\n",
    "        \n",
    "        matched = [t for t in query_terms if t in vocabulary]\n",
    "        unmatched = [t for t in query_terms if t not in vocabulary]\n",
    "        \n",
    "        matched_ngrams = []\n",
    "        for n in range(2, self.ngram_range[1] + 1):\n",
    "            for i in range(len(query_terms) - n + 1):\n",
    "                ngram = ' '.join(query_terms[i:i+n])\n",
    "                if ngram in vocabulary:\n",
    "                    matched_ngrams.append(ngram)\n",
    "        \n",
    "        return {\n",
    "            'original_query': query,\n",
    "            'processed_query': processed_query,\n",
    "            'matched_terms': matched,\n",
    "            'matched_ngrams': matched_ngrams,\n",
    "            'unmatched_terms': unmatched\n",
    "        }\n",
    "    \n",
    "    def get_top_terms_for_document(self, doc_id: int, top_k: int = 10) -> List[Tuple[str, float]]:\n",
    "        if doc_id not in self.id_to_index:\n",
    "            raise ValueError(f\"document id {doc_id} not found\")\n",
    "        \n",
    "        idx = self.id_to_index[doc_id]\n",
    "        feature_names = self.vectorizer.get_feature_names_out()\n",
    "        doc_vector = self.tfidf_matrix[idx].toarray().flatten()\n",
    "        \n",
    "        top_indices = np.argsort(doc_vector)[::-1][:top_k]\n",
    "        \n",
    "        return [(feature_names[i], float(doc_vector[i])) for i in top_indices if doc_vector[i] > 0]\n",
    "    \n",
    "    def save(self, filepath: str) -> None:\n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'vectorizer': self.vectorizer,\n",
    "                'tfidf_matrix': self.tfidf_matrix,\n",
    "                'document_ids': self.document_ids,\n",
    "                'id_to_index': self.id_to_index,\n",
    "                'preprocessor': self.preprocessor,\n",
    "                'ngram_range': self.ngram_range,\n",
    "                'max_features': self.max_features\n",
    "            }, f)\n",
    "    \n",
    "    def load(self, filepath: str) -> None: # loading fitted model previously saved\n",
    "        with open(filepath, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        \n",
    "        self.vectorizer = data['vectorizer']\n",
    "        self.tfidf_matrix = data['tfidf_matrix']\n",
    "        self.document_ids = data['document_ids']\n",
    "        self.id_to_index = data.get('id_to_index', {doc_id: idx for idx, doc_id in enumerate(self.document_ids)})\n",
    "        self.preprocessor = data['preprocessor']\n",
    "        self.ngram_range = data['ngram_range']\n",
    "        self.max_features = data['max_features']\n",
    "        self.is_fitted = True\n",
    "\n",
    "# UTILITY FUNCTIONS\n",
    "def display_search_results(results, metadata_df, corpus_df, query, show_snippet=True):\n",
    "    if not results:\n",
    "        print(\"No results found\")\n",
    "        return\n",
    "    \n",
    "    for rank, (recipe_id, score) in enumerate(results, 1):\n",
    "        meta_row = metadata_df[metadata_df['recipe_id'] == recipe_id]\n",
    "        corpus_row = corpus_df[corpus_df['recipe_id'] == recipe_id]\n",
    "        \n",
    "        if len(meta_row) == 0:\n",
    "            continue\n",
    "            \n",
    "        meta = meta_row.iloc[0]\n",
    "        \n",
    "        print(f\"\\n[{rank}] {meta['recipe_name']}\")\n",
    "        print(f\"Score: {score:.4f}\")\n",
    "        print(f\"Cooking Time: {meta['cooking_time']} min | \"\n",
    "              f\"Ingredients: {meta['num_ingredients']} | \"\n",
    "              f\"Steps: {meta['num_steps']}\")\n",
    "        \n",
    "        if show_snippet and len(corpus_row) > 0:\n",
    "            doc = corpus_row.iloc[0]['document']\n",
    "            snippet = doc[:200] + \"...\" if len(doc) > 200 else doc\n",
    "            print(f\"Preview: {snippet}\")\n",
    "        \n",
    "        if pd.notna(meta['description']) and str(meta['description']) != 'nan':\n",
    "            desc = str(meta['description'])[:150]\n",
    "            print(f\"Description: {desc}...\")\n",
    "\n",
    "\n",
    "def analyze_query_matching(search_engine: TFIDFSearchEngine, query: str) -> None:\n",
    "    \n",
    "    analysis = search_engine.get_query_terms(query)\n",
    "    \n",
    "    print(f\"\\n--- Query Analysis for: '{query}' ---\")\n",
    "    print(f\"Processed query: '{analysis['processed_query']}'\")\n",
    "    print(f\"Matched unigrams: {analysis['matched_terms']}\")\n",
    "    print(f\"Matched n-grams: {analysis['matched_ngrams']}\")\n",
    "    print(f\"Unmatched terms: {analysis['unmatched_terms']}\")\n",
    "\n",
    "\n",
    "\n",
    "# BUILD TF-IDF SEARCH ENGINE\n",
    "tfidf_engine = TFIDFSearchEngine(\n",
    "    ngram_range=(1, 2),\n",
    "    max_features=50000,\n",
    "    min_df=3, \n",
    "    max_df=0.90,\n",
    "    sublinear_tf=True\n",
    ")\n",
    "\n",
    "tfidf_engine.fit(\n",
    "    documents=corpus_df['document'],\n",
    "    document_ids=corpus_df['recipe_id'].tolist(),\n",
    "    preprocess=True\n",
    ")\n",
    "\n",
    "tfidf_engine.save(\"tfidf_search_engine.pkl\")\n",
    "\n",
    "\n",
    "# TEST TF-IDF ENGINE\n",
    "test_queries = [\n",
    "    \"chocolate cake\",\n",
    "    \"\",\n",
    "    \"chicken soup\",\n",
    "    \"comfort food for a rainy day\",\n",
    "    \"healthy dinner after gym\",\n",
    "    \"quick and easy breakfast\",\n",
    "    \"romantic dinner for two\",\n",
    "    \"light summer salad\",\n",
    "]\n",
    "\n",
    "for query in test_queries[:4]:  # test first 4 queries\n",
    "    analyze_query_matching(tfidf_engine, query)\n",
    "    results = tfidf_engine.search(query, top_k=3)\n",
    "    display_search_results(results, metadata_df, corpus_df, query, show_snippet=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bc26be",
   "metadata": {},
   "source": [
    "### Neural embeddings search engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e69a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sequence length (tokens): 384\n",
      "Embedding dimension: 384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing documents: 100%|██████████| 93410/93410 [00:01<00:00, 52605.93it/s]\n",
      "Batches: 100%|██████████| 1460/1460 [57:44<00:00,  2.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (93410, 384)\n",
      "\n",
      "Embeddings search engine statistics:\n",
      "Documents: 93,410\n",
      "Embedding dimension: 384\n",
      "\n",
      "EMBEDDINGS RESULTS FOR: 'chocolate cake'\n",
      "\n",
      "[1] the ultimate fudgy chocolate cake of love\n",
      "Similarity Score: 0.6525\n",
      "Cooking Time: 70 min | Ingredients: 8 | Steps: 5\n",
      "Tags: time-to-make, course, preparation, occasion, desserts, easy, dinner-party, holiday-event, kid-friend...\n",
      "Description: recently a co-worker had a birthday, and someone made this cake for them.  i was lucky enough to snag a piece, and fell in love.  it is very moist, de...\n",
      "\n",
      "[2] heavenly chocolate cake\n",
      "Similarity Score: 0.6173\n",
      "Cooking Time: 50 min | Ingredients: 8 | Steps: 11\n",
      "Tags: 60-minutes-or-less, time-to-make, course, preparation, occasion, for-large-groups, desserts, oven, e...\n",
      "Description: this is a sinfully rich cake that would be wonderful for a birthday party. i found it in our local newspaper years ago and have make it numrous times....\n",
      "\n",
      "[3] ultra moist chocolate cake\n",
      "Similarity Score: 0.6097\n",
      "Cooking Time: 11 min | Ingredients: 10 | Steps: 5\n",
      "Tags: 15-minutes-or-less, time-to-make, main-ingredient, preparation, chocolate...\n",
      "Description: a fairly chocolaty cake it is an amazing dessert for parties...\n",
      "\n",
      "EMBEDDINGS RESULTS FOR: 'pasta carbonara'\n",
      "\n",
      "[1] my fake pasta carbonara\n",
      "Similarity Score: 0.6664\n",
      "Cooking Time: 205 min | Ingredients: 10 | Steps: 17\n",
      "Tags: weeknight, time-to-make, course, main-ingredient, cuisine, preparation, main-dish, pasta, european, ...\n",
      "Description: i know it's probably not true pasta carbonara, but i love this. it's a bit high on the prep time but well worth the effort. oh, and it's not for the f...\n",
      "\n",
      "[2] carbonara al franco\n",
      "Similarity Score: 0.6626\n",
      "Cooking Time: 45 min | Ingredients: 11 | Steps: 9\n",
      "Tags: 60-minutes-or-less, time-to-make, course, main-ingredient, cuisine, preparation, for-1-or-2, main-di...\n",
      "Description: from la times....\n",
      "\n",
      "[3] spaghetti a la carbonara\n",
      "Similarity Score: 0.6588\n",
      "Cooking Time: 25 min | Ingredients: 11 | Steps: 11\n",
      "Tags: 30-minutes-or-less, time-to-make, main-ingredient, preparation, pasta, pasta-rice-and-grains, spaghe...\n",
      "Description: this is rachel ray\"s recipe...\n",
      "\n",
      "EMBEDDINGS RESULTS FOR: 'chicken soup'\n",
      "\n",
      "[1] super simple and speedy chicken soup\n",
      "Similarity Score: 0.6952\n",
      "Cooking Time: 15 min | Ingredients: 6 | Steps: 5\n",
      "Tags: 15-minutes-or-less, time-to-make, course, main-ingredient, preparation, occasion, lunch, soups-stews...\n",
      "Description: need chicken soup fast and don't have time to cook that bird for hours? this has happened to us plenty of times, when someone has come home in the moo...\n",
      "\n",
      "[2] easy chicken and veggie soup\n",
      "Similarity Score: 0.6648\n",
      "Cooking Time: 30 min | Ingredients: 7 | Steps: 8\n",
      "Tags: 30-minutes-or-less, time-to-make, course, preparation, soups-stews...\n",
      "Description: this is my husband's favorite chicken soup....\n",
      "\n",
      "[3] baked chicken soup\n",
      "Similarity Score: 0.6539\n",
      "Cooking Time: 45 min | Ingredients: 13 | Steps: 18\n",
      "Tags: 60-minutes-or-less, time-to-make, course, preparation, soups-stews...\n",
      "Description: this soup is my version of demos' restaurant's baked chicken soup. i found a recipe for basic chicken soup and changed it to mimic what we were lookin...\n",
      "\n",
      "EMBEDDINGS RESULTS FOR: 'comfort food for a rainy day'\n",
      "\n",
      "[1] lentil soup for miserable rainy days\n",
      "Similarity Score: 0.4749\n",
      "Cooking Time: 120 min | Ingredients: 14 | Steps: 33\n",
      "Tags: time-to-make, course, main-ingredient, preparation, soups-stews, beans, lentils, 4-hours-or-less...\n",
      "Description: this is part of my \"make after work\" soup collection that i reserve for rainy and/or miserable days. it is rich, tasty, and so soothing! you need abou...\n",
      "\n",
      "[2] cozy orzo and white bean soup\n",
      "Similarity Score: 0.4408\n",
      "Cooking Time: 40 min | Ingredients: 11 | Steps: 8\n",
      "Tags: 60-minutes-or-less, time-to-make, course, preparation, soups-stews, stocks, dietary...\n",
      "Description: i say \"cozy\" cause this is my favorite comfort soup, it's so rich in flavour you'll want to make it every time it rains!...\n",
      "\n",
      "[3] turkish lentil soup\n",
      "Similarity Score: 0.4393\n",
      "Cooking Time: 30 min | Ingredients: 12 | Steps: 12\n",
      "Tags: 30-minutes-or-less, time-to-make, course, main-ingredient, preparation, occasion, clear-soups, soups...\n",
      "Description: i am on a bit of a soup kick right now in the quest to eat healthier. this is a recipe from my grandmother's recipe box. i always love grandma's cooki...\n",
      "\n",
      "EMBEDDINGS RESULTS FOR: 'healthy dinner after gym'\n",
      "\n",
      "[1] the answer  protein shake\n",
      "Similarity Score: 0.4825\n",
      "Cooking Time: 4 min | Ingredients: 4 | Steps: 4\n",
      "Tags: 15-minutes-or-less, time-to-make, course, main-ingredient, cuisine, preparation, north-american, for...\n",
      "Description: so what's the question? want to lose weight? want to supplement your workout to tone muscle? want to trick your kids into drinking something that tast...\n",
      "\n",
      "[2] amy s workout winner\n",
      "Similarity Score: 0.4822\n",
      "Cooking Time: 10 min | Ingredients: 4 | Steps: 2\n",
      "Tags: 15-minutes-or-less, time-to-make, course, main-ingredient, preparation, for-1-or-2, 5-ingredients-or...\n",
      "Description: i workout 5 days a week and when i come home i am ready to devour everything in sight but the usual 2 boiled eggs and whole wheat toast just dont cut ...\n",
      "\n",
      "[3] situ s chicken salad\n",
      "Similarity Score: 0.4576\n",
      "Cooking Time: 35 min | Ingredients: 8 | Steps: 7\n",
      "Tags: 60-minutes-or-less, time-to-make, course, main-ingredient, cuisine, preparation, occasion, lunch, po...\n",
      "Description: this is the way my grandmother makes use of leftover chicken. add more or less lemon juice to taste. you can also add diced white onion if desired....\n",
      "\n",
      "Query-to-Query Similarity Matrix:\n",
      "                      healthy dinner nutritious meal       diet food    comfort food     hearty meal     cozy dinner\n",
      "healthy dinner                 1.000           0.731           0.568           0.536           0.643           0.630\n",
      "nutritious meal                0.731           1.000           0.673           0.554           0.660           0.442\n",
      "diet food                      0.568           0.673           1.000           0.526           0.525           0.291\n",
      "comfort food                   0.536           0.554           0.526           1.000           0.618           0.582\n",
      "hearty meal                    0.643           0.660           0.525           0.618           1.000           0.526\n",
      "cozy dinner                    0.630           0.442           0.291           0.582           0.526           1.000\n"
     ]
    }
   ],
   "source": [
    "# Check for GPU (shorter run time needed)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "if device == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "\n",
    "# DOCUMENT PREPARER\n",
    "class DocumentPreparer: \n",
    "    def __init__(self, max_length_words: int = None):\n",
    "        self.max_length_words = max_length_words \n",
    "        # turns recipe dataframe rows into optimised strings for embeddings (name, tag, content)\n",
    "    \n",
    "    def prepare_document(self, row: pd.Series) -> str: \n",
    "        # processes a sigle string by extracting and formatting relevant fields\n",
    "        parts = []\n",
    "        \n",
    "        # Recipe name\n",
    "        recipe_name = row.get('recipe_name', '')\n",
    "        if pd.notna(recipe_name) and str(recipe_name).strip():\n",
    "            name = str(recipe_name).strip()\n",
    "            parts.append(f\"Recipe: {name}\")\n",
    "        \n",
    "        # Tags (necessary for semantic matching)\n",
    "        tags = row.get('tags', '')\n",
    "        if pd.notna(tags) and str(tags).strip():\n",
    "            tags_clean = str(tags).strip().replace(',', ', ')\n",
    "            parts.append(f\"Tags: {tags_clean}\")\n",
    "        \n",
    "        # Full document content\n",
    "        document = row.get('document', '')\n",
    "        if pd.notna(document) and str(document).strip():\n",
    "            doc = str(document).strip()\n",
    "            if self.max_length_words is not None:\n",
    "                words = doc.split()\n",
    "                if len(words) > self.max_length_words:\n",
    "                    doc = ' '.join(words[:self.max_length_words])\n",
    "            parts.append(doc)\n",
    "        \n",
    "        return ' '.join(parts) # returns one string\n",
    "    \n",
    "    def prepare_batch(self, df: pd.DataFrame, show_progress: bool = True) -> List[str]: \n",
    "        # elaborates the entire dataframe iterating row by row and for each it gives a formatted string ready for embedding\n",
    "        prepared = []\n",
    "        iterator = tqdm(df.iterrows(), total=len(df), desc=\"Preparing documents\", disable=not show_progress)\n",
    "        \n",
    "        for _, row in iterator:\n",
    "            prepared.append(self.prepare_document(row))\n",
    "        \n",
    "        return prepared \n",
    "\n",
    "\n",
    "\n",
    "# EMBEDDINGS SEARCH ENGINE \n",
    "\n",
    "class EmbeddingsSearchEngine: \n",
    "    # converts documents and queries into dense vector representations (embeddings) using a SentenceTransformer model\n",
    "    \n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\", batch_size: int = 64, max_seq_length: int = 384):\n",
    "        self.model_path = model_name\n",
    "        self.batch_size = batch_size\n",
    "        self.model = SentenceTransformer(self.model_path, device=device)\n",
    "\n",
    "        # Set the maximum sequence length (tokens) used by the model tokenizer for truncation\n",
    "        self.model.max_seq_length = max_seq_length\n",
    "        print(f\"Max sequence length (tokens): {self.model.max_seq_length}\")\n",
    "\n",
    "        # embedding dimension\n",
    "        self.embedding_dim = self.model.get_sentence_embedding_dimension()\n",
    "        print(f\"Embedding dimension: {self.embedding_dim}\")\n",
    "\n",
    "        self.embeddings: Optional[np.ndarray] = None  \n",
    "        self.document_ids: Optional[List] = None\n",
    "        self.id_to_index: Dict[int, int] = {}\n",
    "        self.is_fitted = False\n",
    "\n",
    "        self.doc_preparer = DocumentPreparer(max_length_words=None)  \n",
    "\n",
    "    def fit(self, documents: Union[pd.DataFrame, List[str]], document_ids: Optional[List] = None, show_progress: bool = True) -> None:\n",
    "        # calculates embeddings for all documents in the corpus\n",
    "\n",
    "        if isinstance(documents, pd.DataFrame):\n",
    "            doc_list = self.doc_preparer.prepare_batch(documents, show_progress=show_progress)\n",
    "            if document_ids is None and 'recipe_id' in documents.columns:\n",
    "                document_ids = documents['recipe_id'].tolist()\n",
    "        elif isinstance(documents, pd.Series):\n",
    "            doc_list = documents.tolist()\n",
    "        else:\n",
    "            doc_list = list(documents)\n",
    "\n",
    "        # Store document IDs\n",
    "        if document_ids is not None:\n",
    "            self.document_ids = list(document_ids)\n",
    "        else:\n",
    "            self.document_ids = list(range(len(doc_list)))\n",
    "\n",
    "        # O(1) lookup\n",
    "        self.id_to_index = {doc_id: idx for idx, doc_id in enumerate(self.document_ids)}\n",
    "\n",
    "        # Compute embeddings in batches\n",
    "        self.embeddings = self.model.encode(\n",
    "            doc_list,\n",
    "            batch_size=self.batch_size,\n",
    "            show_progress_bar=show_progress,\n",
    "            convert_to_numpy=True,\n",
    "            normalize_embeddings=True  # important for cosine via dot-product\n",
    "        ).astype(np.float32)\n",
    "\n",
    "        print(f\"Embeddings shape: {self.embeddings.shape}\")\n",
    "\n",
    "        self.is_fitted = True\n",
    "\n",
    "        print(f\"\\nEmbeddings search engine statistics:\")\n",
    "        print(f\"Documents: {len(self.document_ids):,}\")\n",
    "        print(f\"Embedding dimension: {self.embedding_dim}\")\n",
    "\n",
    "    def encode_query(self, query: str) -> np.ndarray:\n",
    "        embedding = self.model.encode(query, convert_to_numpy=True, normalize_embeddings=True).astype(np.float32)\n",
    "        return embedding\n",
    "\n",
    "    def _top_k_from_scores(self, scores: np.ndarray, top_k: int) -> List[int]:\n",
    "        n = scores.shape[0]\n",
    "        if n == 0:\n",
    "            return []\n",
    "        k = min(top_k, n)\n",
    "\n",
    "        # argpartition for speed, then sort those k\n",
    "        idx_part = np.argpartition(scores, -k)[-k:]\n",
    "        idx_sorted = idx_part[np.argsort(scores[idx_part])[::-1]]\n",
    "        return idx_sorted.tolist()\n",
    "\n",
    "    def search(self, query: str, top_k: int = 10) -> List[Tuple[int, float]]:\n",
    "        # finds recipes similar to a query by calculating similarity score between embeddings and wueries\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Model not fitted, do fit() first\") # important\n",
    "\n",
    "        if not query or not query.strip():\n",
    "            return []\n",
    "\n",
    "        q = self.encode_query(query)\n",
    "        # cosine similarity because both sides are normalized\n",
    "        scores = self.embeddings @ q \n",
    "\n",
    "        top_indices = self._top_k_from_scores(scores, top_k)\n",
    "\n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            doc_id = self.document_ids[idx]\n",
    "            results.append((doc_id, float(scores[idx])))\n",
    "        return results\n",
    "\n",
    "    def search_batch(self, queries: List[str], top_k: int = 10) -> Dict[str, List[Tuple[int, float]]]:\n",
    "        # calculates all similarities between queries and documents with one matrix product\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Model not fitted, fit() first\")\n",
    "\n",
    "        valid_queries = [q for q in queries if q and q.strip()]\n",
    "        if not valid_queries:\n",
    "            return {q: [] for q in queries}\n",
    "\n",
    "        query_embeddings = self.model.encode(valid_queries, batch_size=self.batch_size, convert_to_numpy=True, normalize_embeddings=True).astype(np.float32)  # (Q, D)\n",
    "\n",
    "        # (Q, N) cosine similarities via dot-product\n",
    "        score_matrix = query_embeddings @ self.embeddings.T\n",
    "\n",
    "        results: Dict[str, List[Tuple[int, float]]] = {}\n",
    "        for i, query in enumerate(valid_queries):\n",
    "            scores = score_matrix[i]\n",
    "            top_indices = self._top_k_from_scores(scores, top_k)\n",
    "            results[query] = [(self.document_ids[idx], float(scores[idx])) for idx in top_indices]\n",
    "\n",
    "        for query in queries:\n",
    "            if query not in results:\n",
    "                results[query] = []\n",
    "\n",
    "        return results\n",
    "\n",
    "    def get_similar_recipes(self, recipe_id: int, top_k: int = 10) -> List[Tuple[int, float]]:\n",
    "        # gets recipes similar ro a recipe, not a query\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Model not fitted do fit() first\")\n",
    "\n",
    "        if recipe_id not in self.id_to_index:\n",
    "            raise ValueError(f\"Recipe ID {recipe_id} not found in index\")\n",
    "\n",
    "        idx = self.id_to_index[recipe_id]\n",
    "        v = self.embeddings[idx] \n",
    "\n",
    "        scores = self.embeddings @ v \n",
    "        scores[idx] = -np.inf  # exclude self by setting its score to -infinity\n",
    "\n",
    "        top_indices = self._top_k_from_scores(scores, top_k)\n",
    "\n",
    "        return [(self.document_ids[i], float(scores[i])) for i in top_indices]\n",
    "    \n",
    "\n",
    "    def save(self, filepath: str) -> None:\n",
    "        # saves embeddings \n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'embeddings': self.embeddings,\n",
    "                'document_ids': self.document_ids,\n",
    "                'id_to_index': self.id_to_index,\n",
    "                'model_path': self.model_path,\n",
    "                'embedding_dim': self.embedding_dim\n",
    "            }, f)\n",
    "\n",
    "    def load(self, filepath: str) -> None:\n",
    "        # loads embeddings\n",
    "        with open(filepath, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "\n",
    "        self.embeddings = data['embeddings'].astype(np.float32)\n",
    "        self.document_ids = data['document_ids']\n",
    "        self.id_to_index = data.get(\n",
    "            'id_to_index',\n",
    "            {doc_id: idx for idx, doc_id in enumerate(self.document_ids)}\n",
    "        )\n",
    "        self.embedding_dim = data['embedding_dim']\n",
    "\n",
    "        self.is_fitted = True\n",
    "\n",
    "# UTILITY FUNCTIONS \n",
    "def display_search_results_embeddings(results: List[Tuple[int, float]], \n",
    "                                      metadata_df: pd.DataFrame, \n",
    "                                      corpus_df: pd.DataFrame, \n",
    "                                      query: str, \n",
    "                                      engine_name: str = \"EMBEDDINGS\") -> None:\n",
    "    \n",
    "    print(f\"\\n{engine_name} RESULTS FOR: '{query}'\")\n",
    "    \n",
    "    if not results:\n",
    "        print(\"No results found\")\n",
    "        return\n",
    "    \n",
    "    for rank, (recipe_id, score) in enumerate(results, 1):\n",
    "        meta_row = metadata_df[metadata_df['recipe_id'] == recipe_id]\n",
    "        \n",
    "        if len(meta_row) == 0:\n",
    "            continue\n",
    "            \n",
    "        meta = meta_row.iloc[0]\n",
    "        \n",
    "        print(f\"\\n[{rank}] {meta['recipe_name']}\")\n",
    "        print(f\"Similarity Score: {score:.4f}\")\n",
    "        print(f\"Cooking Time: {meta['cooking_time']} min | \"\n",
    "              f\"Ingredients: {meta['num_ingredients']} | \"\n",
    "              f\"Steps: {meta['num_steps']}\")\n",
    "        \n",
    "        # Show tags if available\n",
    "        corpus_row = corpus_df[corpus_df['recipe_id'] == recipe_id]\n",
    "        if len(corpus_row) > 0:\n",
    "            tags = corpus_row.iloc[0].get('tags', '')\n",
    "            if pd.notna(tags) and str(tags) != 'nan':\n",
    "                tags_preview = str(tags)[:100]\n",
    "                print(f\"Tags: {tags_preview}...\")\n",
    "        \n",
    "        # Show description\n",
    "        if pd.notna(meta['description']) and str(meta['description']) != 'nan':\n",
    "            desc = str(meta['description'])[:150]\n",
    "            print(f\"Description: {desc}...\")\n",
    "\n",
    "\n",
    "\n",
    "# BUILD EMBEDDINGS SEARCH ENGINE\n",
    "embeddings_engine = EmbeddingsSearchEngine(model_name='all-MiniLM-L6-v2', batch_size=64)\n",
    "embeddings_engine.fit(documents=corpus_df, document_ids=corpus_df['recipe_id'].tolist(), show_progress=True)\n",
    "embeddings_engine.save(\"embeddings_search_engine.pkl\")\n",
    "\n",
    "\n",
    "# TEST EMBEDDINGS ENGINE\n",
    "test_queries_embeddings = [\n",
    "    # Simple keyword queries\n",
    "    \"chocolate cake\",\n",
    "    \"pasta carbonara\",\n",
    "    \"chicken soup\",\n",
    "    \n",
    "    # Semantic/high-level queries\n",
    "    \"comfort food for a rainy day\",\n",
    "    \"healthy dinner after gym\",\n",
    "    \"quick and easy breakfast\",\n",
    "    \"romantic dinner for two\",\n",
    "    \"light summer salad\",\n",
    "    \"warm winter soup\",\n",
    "    \"kid friendly lunch\",\n",
    "    \"low carb vegetarian\",\n",
    "    \n",
    "    # Abstract/mood-based queries\n",
    "    \"something sweet and indulgent\",\n",
    "    \"meal prep for the week\",\n",
    "    \"impressive dish for guests\",\n",
    "    \"nostalgic childhood favorite\"\n",
    "]\n",
    "\n",
    "# Test first 5 queries\n",
    "for query in test_queries_embeddings[:5]:\n",
    "    results = embeddings_engine.search(query, top_k=3) # returns 3 most similar recipes to query\n",
    "    display_search_results_embeddings(results, metadata_df, corpus_df, query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5fea88",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9735d64a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total queries: 25\n",
      "  keyword: 10\n",
      "  semantic: 10\n",
      "  dietary_cuisine: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing queries:  12%|█▏        | 3/25 [00:00<00:05,  3.70it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing queries: 100%|██████████| 25/25 [00:07<00:00,  3.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Statistics (Top-10 results):\n",
      "Average overlap: 0.9 recipes (9.2%)\n",
      "Min overlap: 0\n",
      "Max overlap: 3\n",
      "\n",
      "Overlap by Query Category:\n",
      "  keyword             : 14.0% average overlap\n",
      "  semantic            : 5.0% average overlap\n",
      "  dietary_cuisine     : 8.0% average overlap\n",
      "\n",
      "Per-Query Overlap:\n",
      "Query                                    Category           Overlap\n",
      "chocolate cake                           keyword              1/10\n",
      "chicken soup                             keyword              2/10\n",
      "pasta carbonara                          keyword              2/10\n",
      "banana bread                             keyword              1/10\n",
      "grilled salmon                           keyword              1/10\n",
      "caesar salad                             keyword              3/10\n",
      "beef stew                                keyword              0/10\n",
      "apple pie                                keyword              2/10\n",
      "garlic bread                             keyword              2/10\n",
      "fried rice                               keyword              0/10\n",
      "comfort food for a rainy day             semantic             1/10\n",
      "healthy dinner after gym                 semantic             0/10\n",
      "quick weeknight meal                     semantic             0/10\n",
      "romantic dinner for two                  semantic             1/10\n",
      "impressive dish for guests               semantic             0/10\n",
      "light summer meal                        semantic             1/10\n",
      "warm cozy winter food                    semantic             1/10\n",
      "kid friendly lunch                       semantic             1/10\n",
      "lazy sunday breakfast                    semantic             0/10\n",
      "something sweet and indulgent            semantic             0/10\n",
      "vegan dessert                            dietary_cuisine      0/10\n",
      "gluten free dinner                       dietary_cuisine      2/10\n",
      "italian pasta dish                       dietary_cuisine      0/10\n",
      "asian stir fry                           dietary_cuisine      2/10\n",
      "low carb meal                            dietary_cuisine      0/10\n",
      "\n",
      "SAMPLE RESULTS COMPARISON\n",
      "\n",
      "QUERY: 'chocolate cake' (keyword)\n",
      "Overlap: 1/10 recipes in common\n",
      "Rank   TF-IDF Result                       Embeddings Result                  \n",
      "1      store bought chocolate cake and m   the ultimate fudgy chocolate cake   \n",
      "2      cake mix chocolate cookies          heavenly chocolate cake             \n",
      "3      miracle whip chocolate cake         ultra moist chocolate cake          \n",
      "4      sunflower centerpiece               best ever chocolate cake   herita   \n",
      "5      reese s cup chocolate cake          chocolate thunder cake              \n",
      "\n",
      "QUERY: 'comfort food for a rainy day' (semantic)\n",
      "Overlap: 1/10 recipes in common\n",
      "Rank   TF-IDF Result                       Embeddings Result                  \n",
      "1      janet s banana bread                lentil soup for miserable rainy d   \n",
      "2      no bake butterscotch drops          cozy orzo and white bean soup       \n",
      "3      rainy day yellow cake               turkish lentil soup                 \n",
      "4      chicken italia soup                 chilled lentil soup with spinach    \n",
      "5      coffee nudge                        turkey soup with egg  noodles and   \n",
      "\n",
      "QUERY: 'vegan dessert' (dietary_cuisine)\n",
      "Overlap: 0/10 recipes in common\n",
      "Rank   TF-IDF Result                       Embeddings Result                  \n",
      "1      cake                                vegan berry pie                     \n",
      "2      decadent vegan chocolate mousse     vegan chocolate mocha pie           \n",
      "3      vegan chicken salad                 vegan fruit butter bars             \n",
      "4      vegan ice cream                     vegan coconut cream pie filling     \n",
      "5      vegan graham cracker crust  for v   no bake vegan banoffee pie          \n",
      "\n",
      "Total judgments needed: 250\n",
      "  - 25 queries × 5 results × 2 engines = 250\n",
      "\n",
      "ANALYSIS OF MANUAL EVALUATIONS\n",
      "\n",
      "OVERALL RESULTS\n",
      "Average Precision@5:\n",
      "TF-IDF:     0.808 (80.8%)\n",
      "Embeddings: 0.920 (92.0%)\n",
      "Difference: +0.112\n",
      "\n",
      "RESULTS BY QUERY CATEGORY\n",
      "Category               TF-IDF P@5    Embed P@5       Diff       Winner\n",
      "keyword                     0.900        0.960     +0.060   Embeddings\n",
      "semantic                    0.660        0.840     +0.180   Embeddings\n",
      "dietary_cuisine             0.920        1.000     +0.080   Embeddings\n",
      "\n",
      "PER-QUERY BREAKDOWN\n",
      "Query                                        TF-IDF      Embed       Winner\n",
      "chocolate cake                                 0.60       0.80   Embeddings\n",
      "chicken soup                                   1.00       0.80       TF-IDF\n",
      "pasta carbonara                                1.00       1.00          Tie\n",
      "banana bread                                   1.00       1.00          Tie\n",
      "grilled salmon                                 0.60       1.00   Embeddings\n",
      "caesar salad                                   1.00       1.00          Tie\n",
      "beef stew                                      1.00       1.00          Tie\n",
      "apple pie                                      1.00       1.00          Tie\n",
      "garlic bread                                   0.80       1.00   Embeddings\n",
      "fried rice                                     1.00       1.00          Tie\n",
      "comfort food for a rainy day                   0.40       1.00   Embeddings\n",
      "healthy dinner after gym                       0.00       0.80   Embeddings\n",
      "quick weeknight meal                           0.60       1.00   Embeddings\n",
      "romantic dinner for two                        1.00       1.00          Tie\n",
      "impressive dish for guests                     0.80       0.60       TF-IDF\n",
      "light summer meal                              0.80       0.80          Tie\n",
      "warm cozy winter food                          0.80       0.80          Tie\n",
      "kid friendly lunch                             0.80       1.00   Embeddings\n",
      "lazy sunday breakfast                          0.60       0.80   Embeddings\n",
      "something sweet and indulgent                  0.80       0.60       TF-IDF\n",
      "vegan dessert                                  0.80       1.00   Embeddings\n",
      "gluten free dinner                             1.00       1.00          Tie\n",
      "italian pasta dish                             0.80       1.00   Embeddings\n",
      "asian stir fry                                 1.00       1.00          Tie\n",
      "low carb meal                                  1.00       1.00          Tie\n",
      "\n",
      "WIN/LOSS SUMMARY\n",
      "----------------------------------------\n",
      "TF-IDF wins:       3 queries\n",
      "Embeddings wins:  10 queries\n",
      "Ties:             12 queries\n",
      "\n",
      "DIFFERENCES BETWEEN ENGINES\n",
      "\n",
      "1. RESULT OVERLAP:\n",
      "- Average overlap in top-10: 9.2%\n",
      "- so 90.8% of results are different\n",
      "\n",
      "2. BY QUERY TYPE:\n",
      "- keyword: 14.0% overlap\n",
      "- semantic: 5.0% overlap\n",
      "- dietary_cuisine: 8.0% overlap\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# QUERIES\n",
    "evaluation_queries = {\n",
    "    'keyword': [\n",
    "        \"chocolate cake\",\n",
    "        \"chicken soup\",\n",
    "        \"pasta carbonara\",\n",
    "        \"banana bread\",\n",
    "        \"grilled salmon\",\n",
    "        \"caesar salad\",\n",
    "        \"beef stew\",\n",
    "        \"apple pie\",\n",
    "        \"garlic bread\",\n",
    "        \"fried rice\"\n",
    "    ],\n",
    "    \n",
    "    'semantic': [\n",
    "        \"comfort food for a rainy day\",\n",
    "        \"healthy dinner after gym\",\n",
    "        \"quick weeknight meal\",\n",
    "        \"romantic dinner for two\",\n",
    "        \"impressive dish for guests\",\n",
    "        \"light summer meal\",\n",
    "        \"warm cozy winter food\",\n",
    "        \"kid friendly lunch\",\n",
    "        \"lazy sunday breakfast\",\n",
    "        \"something sweet and indulgent\"\n",
    "    ],\n",
    "    \n",
    "    'dietary_cuisine': [\n",
    "        \"vegan dessert\",\n",
    "        \"gluten free dinner\",\n",
    "        \"italian pasta dish\",\n",
    "        \"asian stir fry\",\n",
    "        \"low carb meal\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Flatten queries\n",
    "all_queries = []\n",
    "query_categories = {}\n",
    "for category, queries in evaluation_queries.items():\n",
    "    for q in queries:\n",
    "        all_queries.append(q)\n",
    "        query_categories[q] = category\n",
    "\n",
    "print(f\"Total queries: {len(all_queries)}\")\n",
    "for cat, queries in evaluation_queries.items():\n",
    "    print(f\"  {cat}: {len(queries)}\")\n",
    "\n",
    "\n",
    "\n",
    "# RUN BOTH ENGINES AND COLLECT TOP-k RESULTS\n",
    "TOP_K = 10\n",
    "results_data = []\n",
    "\n",
    "for query in tqdm(all_queries, desc=\"Processing queries\"):\n",
    "    category = query_categories[query]\n",
    "    \n",
    "    # Get results from both engines\n",
    "    tfidf_results = tfidf_engine.search(query, top_k=TOP_K)\n",
    "    embed_results = embeddings_engine.search(query, top_k=TOP_K)\n",
    "    \n",
    "    # Extract IDs and scores\n",
    "    tfidf_ids = [r[0] for r in tfidf_results]\n",
    "    tfidf_scores = [r[1] for r in tfidf_results]\n",
    "    embed_ids = [r[0] for r in embed_results]\n",
    "    embed_scores = [r[1] for r in embed_results]\n",
    "    \n",
    "    # Calculate overlap\n",
    "    tfidf_set = set(tfidf_ids)\n",
    "    embed_set = set(embed_ids)\n",
    "    overlap = tfidf_set.intersection(embed_set)\n",
    "    overlap_pct = len(overlap) / TOP_K * 100 if TOP_K > 0 else 0\n",
    "    \n",
    "    results_data.append({\n",
    "        'query': query,\n",
    "        'category': category,\n",
    "        'tfidf_ids': tfidf_ids,\n",
    "        'tfidf_scores': tfidf_scores,\n",
    "        'embed_ids': embed_ids,\n",
    "        'embed_scores': embed_scores,\n",
    "        'overlap_count': len(overlap),\n",
    "        'overlap_pct': overlap_pct\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame\n",
    "results_df = pd.DataFrame(results_data)\n",
    "\n",
    "\n",
    "# OVERLAP ANALYSIS\n",
    "\n",
    "# statistics\n",
    "avg_overlap = results_df['overlap_count'].mean()\n",
    "avg_overlap_pct = results_df['overlap_pct'].mean()\n",
    "\n",
    "print(f\"\\nOverall Statistics (Top-{TOP_K} results):\")\n",
    "print(f\"Average overlap: {avg_overlap:.1f} recipes ({avg_overlap_pct:.1f}%)\")\n",
    "print(f\"Min overlap: {results_df['overlap_count'].min()}\")\n",
    "print(f\"Max overlap: {results_df['overlap_count'].max()}\")\n",
    "\n",
    "# Overlap by category\n",
    "print(f\"\\nOverlap by Query Category:\")\n",
    "for category in evaluation_queries.keys():\n",
    "    cat_data = results_df[results_df['category'] == category]\n",
    "    cat_avg = cat_data['overlap_pct'].mean()\n",
    "    print(f\"  {category}: {cat_avg:.1f}% average overlap\")\n",
    "\n",
    "# Detailed per-query overlap\n",
    "print(f\"\\nPer-Query Overlap:\")\n",
    "print(\"Query Category Overlap\")\n",
    "for _, row in results_df.iterrows():\n",
    "    print(f\"{row['query']} {row['category']} {row['overlap_count']}/{TOP_K}\")\n",
    "\n",
    "\n",
    "\n",
    "# SAMPLE RESULTS\n",
    "print(\"\\nSAMPLE RESULTS COMPARISON\")\n",
    "sample_queries = [\n",
    "    \"chocolate cake\",           # keyword\n",
    "    \"comfort food for a rainy day\",  # semantic\n",
    "    \"vegan dessert\"             # dietary\n",
    "]\n",
    "\n",
    "def get_recipe_name(recipe_id):\n",
    "    match = metadata_df[metadata_df['recipe_id'] == recipe_id]\n",
    "    return match.iloc[0]['recipe_name'] if len(match) > 0 else f\"Recipe {recipe_id}\"\n",
    "\n",
    "for query in sample_queries:\n",
    "    row = results_df[results_df['query'] == query].iloc[0]\n",
    "    print(f\"\\nQUERY: '{query}' ({row['category']})\")\n",
    "    print(f\"Overlap: {row['overlap_count']}/{TOP_K} recipes in common\")\n",
    "    print(\"Rank TF-IDF Result Embeddings Result\")\n",
    "\n",
    "    n = min(5, len(row[\"tfidf_ids\"]), len(row[\"embed_ids\"])) # top 5\n",
    "    for rank in range(n):\n",
    "        tfidf_name = get_recipe_name(row['tfidf_ids'][rank])[:33]\n",
    "        embed_name = get_recipe_name(row['embed_ids'][rank])[:33]\n",
    "\n",
    "        # Mark if same recipe\n",
    "        same = \"same\" if row['tfidf_ids'][rank] == row['embed_ids'][rank] else \"\"\n",
    "\n",
    "        print(f\"{rank+1} {tfidf_name} {embed_name} {same}\")\n",
    "\n",
    "\n",
    "\n",
    "# MANUAL EVALUATION FIlE (creation)\n",
    "eval_template = []\n",
    "for query in all_queries:\n",
    "    row = results_df[results_df['query'] == query].iloc[0]\n",
    "\n",
    "    n_tfidf = min(5, len(row[\"tfidf_ids\"]))\n",
    "    for rank in range(n_tfidf):\n",
    "        eval_template.append({\n",
    "            'query': query,\n",
    "            'category': query_categories[query],\n",
    "            'engine': 'TF-IDF',\n",
    "            'rank': rank + 1,\n",
    "            'recipe_id': row['tfidf_ids'][rank],\n",
    "            'recipe_name': get_recipe_name(row['tfidf_ids'][rank]),\n",
    "            'score': row['tfidf_scores'][rank],\n",
    "            'relevant': '' \n",
    "        })\n",
    "\n",
    "    n_embed = min(5, len(row[\"embed_ids\"]))\n",
    "    for rank in range(n_embed):\n",
    "        eval_template.append({\n",
    "            'query': query,\n",
    "            'category': query_categories[query],\n",
    "            'engine': 'Embeddings',\n",
    "            'rank': rank + 1,\n",
    "            'recipe_id': row['embed_ids'][rank],\n",
    "            'recipe_name': get_recipe_name(row['embed_ids'][rank]),\n",
    "            'score': row['embed_scores'][rank],\n",
    "            'relevant': ''\n",
    "        })\n",
    "\n",
    "eval_template_df = pd.DataFrame(eval_template)\n",
    "eval_template_df.to_csv(\"manual_evaluation_template.csv\", index=False)\n",
    "\n",
    "print(f\"\\nTotal judgments needed: {len(eval_template_df)}\")\n",
    "print(f\"  - {len(all_queries)} queries × 5 results × 2 engines = {len(all_queries) * 5 * 2}\")\n",
    "\n",
    "\n",
    "\n",
    "# ANALYSIS OF MANUAL EVALUATIONS\n",
    "def analyze_manual_evaluations(filepath=\"manual_evaluation_complete.csv\"):\n",
    "    print(\"\\nANALYSIS OF MANUAL EVALUATIONS\")\n",
    "    try:\n",
    "        eval_df = pd.read_csv(filepath)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"\\nFile '{filepath}' not found\")\n",
    "        print(\"Please complete the manual evaluation first.\")\n",
    "        return None\n",
    "\n",
    "    # Check if evaluations are complete\n",
    "    if eval_df['relevant'].isna().any() or (eval_df['relevant'] == '').any():\n",
    "        missing = eval_df['relevant'].isna().sum() + (eval_df['relevant'] == '').sum()\n",
    "        print(f\"\\n{missing} judgments are missing\")\n",
    "        print(\" complete all evaluations\")\n",
    "        return None\n",
    "\n",
    "    # Convert to int\n",
    "    eval_df['relevant'] = eval_df['relevant'].astype(int)\n",
    "\n",
    "    # Calculate Precision@5 for each query-engine pair\n",
    "    precision_results = []\n",
    "\n",
    "    for query in all_queries:\n",
    "        query_data = eval_df[eval_df['query'] == query]\n",
    "        category = query_categories[query]\n",
    "\n",
    "        for engine in ['TF-IDF', 'Embeddings']:\n",
    "            engine_data = query_data[query_data['engine'] == engine]\n",
    "            relevant_count = engine_data['relevant'].sum()\n",
    "            precision = relevant_count / 5\n",
    "\n",
    "            precision_results.append({\n",
    "                'query': query,\n",
    "                'category': category,\n",
    "                'engine': engine,\n",
    "                'relevant_in_top5': relevant_count,\n",
    "                'precision@5': precision\n",
    "            })\n",
    "\n",
    "    precision_df = pd.DataFrame(precision_results)\n",
    "\n",
    "    # Overall comparison\n",
    "    print(\"\\nOVERALL RESULTS\")\n",
    "\n",
    "    tfidf_precision = precision_df[precision_df['engine'] == 'TF-IDF']['precision@5'].mean()\n",
    "    embed_precision = precision_df[precision_df['engine'] == 'Embeddings']['precision@5'].mean()\n",
    "\n",
    "    print(\"Average Precision@5:\")\n",
    "    print(f\"TF-IDF: {tfidf_precision:.3f} ({tfidf_precision*100:.1f}%)\")\n",
    "    print(f\"Embeddings: {embed_precision:.3f} ({embed_precision*100:.1f}%)\")\n",
    "    print(f\"Difference: {embed_precision - tfidf_precision:+.3f}\")\n",
    "\n",
    "    # Comparison by category\n",
    "    print(\"\\nRESULTS BY QUERY CATEGORY\")\n",
    "    print(\"Category TF-IDF P@5 Embed P@5 Diff Winner\")\n",
    "\n",
    "    category_summary = []\n",
    "    for category in evaluation_queries.keys():\n",
    "        cat_data = precision_df[precision_df['category'] == category]\n",
    "        tfidf_p = cat_data[cat_data['engine'] == 'TF-IDF']['precision@5'].mean()\n",
    "        embed_p = cat_data[cat_data['engine'] == 'Embeddings']['precision@5'].mean()\n",
    "        diff = embed_p - tfidf_p # calculate differenxe\n",
    "\n",
    "        if abs(diff) < 0.05:\n",
    "            winner = \"Tie\"\n",
    "        elif diff > 0:\n",
    "            winner = \"Embeddings\"\n",
    "        else:\n",
    "            winner = \"TF-IDF\"\n",
    "\n",
    "        print(f\"{category} {tfidf_p:.3f} {embed_p:.3f} {diff:+.3f} {winner}\")\n",
    "\n",
    "        category_summary.append({\n",
    "            'category': category,\n",
    "            'tfidf_precision': tfidf_p,\n",
    "            'embed_precision': embed_p,\n",
    "            'winner': winner\n",
    "        })\n",
    "\n",
    "    # Per-query breakdown (P@5 for queries)\n",
    "    print(\"\\nPER-QUERY BREAKDOWN\")\n",
    "    print(\"Query TF-IDF Embed Winner\")\n",
    "\n",
    "    query_winners = {'TF-IDF': 0, 'Embeddings': 0, 'Tie': 0}\n",
    "\n",
    "    for query in all_queries:\n",
    "        q_data = precision_df[precision_df['query'] == query]\n",
    "        tfidf_p = q_data[q_data['engine'] == 'TF-IDF']['precision@5'].values[0]\n",
    "        embed_p = q_data[q_data['engine'] == 'Embeddings']['precision@5'].values[0]\n",
    "\n",
    "        if abs(tfidf_p - embed_p) < 0.01:\n",
    "            winner = \"Tie\"\n",
    "        elif tfidf_p > embed_p:\n",
    "            winner = \"TF-IDF\"\n",
    "        else:\n",
    "            winner = \"Embeddings\"\n",
    "\n",
    "        query_winners[winner] += 1\n",
    "        print(f\"{query} {tfidf_p:.2f} {embed_p:.2f} {winner}\")\n",
    "\n",
    "    # Win/Loss summary\n",
    "    print(\"\\nWIN/LOSS SUMMARY\")\n",
    "    print(f\"TF-IDF wins: {query_winners['TF-IDF']} queries\")\n",
    "    print(f\"Embeddings wins: {query_winners['Embeddings']} queries\")\n",
    "    print(f\"Ties: {query_winners['Tie']} queries\")\n",
    "\n",
    "    # Save results\n",
    "    precision_df.to_csv(\"evaluation_results.csv\", index=False)\n",
    "\n",
    "    return {\n",
    "        'precision_df': precision_df,\n",
    "        'tfidf_overall': tfidf_precision,\n",
    "        'embed_overall': embed_precision,\n",
    "        'category_summary': category_summary,\n",
    "        'query_winners': query_winners\n",
    "    }\n",
    "\n",
    "\n",
    "results = analyze_manual_evaluations(\"manual_evaluation_complete.csv\")  # only run after completing file\n",
    "\n",
    "\n",
    "# QUICK VISUAL SUMMARY\n",
    "\n",
    "print(\"\\nDIFFERENCES BETWEEN ENGINES\")\n",
    "print(f\"\"\"\n",
    "1. RESULT OVERLAP:\n",
    "- Average overlap in top-{TOP_K}: {avg_overlap_pct:.1f}%\n",
    "- so {100 - avg_overlap_pct:.1f}% of results are different\n",
    "\n",
    "2. BY QUERY TYPE:\"\"\")\n",
    "for category in evaluation_queries.keys():\n",
    "    cat_data = results_df[results_df['category'] == category]\n",
    "    cat_avg = cat_data['overlap_pct'].mean()\n",
    "    print(f\"- {category}: {cat_avg:.1f}% overlap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5fb9416d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1] cheesy chicken sandwiches\n",
      "Score: 0.2570\n",
      "Cooking Time: 20 min | Ingredients: 5 | Steps: 5\n",
      "Description: a quick and easy meal and a kid pleaser!...\n",
      "\n",
      "[2] 1 pot  4 item   sausage suprise\n",
      "Score: 0.2481\n",
      "Cooking Time: 50 min | Ingredients: 4 | Steps: 6\n",
      "Description: i needed to make a quick easy meal for my kids one day and all i had was some kielbasa sausage, onions, potatoes and carrots.  therefore, i cut them u...\n",
      "\n",
      "[3] hamburger tater tots casserole\n",
      "Score: 0.2337\n",
      "Cooking Time: 80 min | Ingredients: 8 | Steps: 4\n",
      "Description: this is a tasty, easy meal that most kids love! i like to serve this with a warm loaf of bread and a salad....\n",
      "\n",
      "[4] cheap hamburger potato casserole\n",
      "Score: 0.2271\n",
      "Cooking Time: 25 min | Ingredients: 3 | Steps: 7\n",
      "Description: this is a super quick, inexpensive, and easy meal to make and it is kid friendly too!  it's comfort food at it's best!...\n",
      "\n",
      "[5] grilled pork chops packets\n",
      "Score: 0.1788\n",
      "Cooking Time: 24 min | Ingredients: 7 | Steps: 11\n",
      "Description: a fun and easy recipe for camping or for the back porch.  i like the classic green pepper/onion flavor, and chops are always moist.  sauce goes great ...\n",
      "\n",
      "EMBEDDINGS RESULTS FOR: 'easy meal for kids'\n",
      "\n",
      "[1] lunch on a stick  for kids\n",
      "Similarity Score: 0.6238\n",
      "Cooking Time: 15 min | Ingredients: 10 | Steps: 1\n",
      "Tags: 15-minutes-or-less, time-to-make, course, main-ingredient, preparation, occasion, for-1-or-2, lunch,...\n",
      "Description: novel and nutritious way of feeding your kids lunch at school....\n",
      "\n",
      "[2] eat your bowl\n",
      "Similarity Score: 0.6109\n",
      "Cooking Time: 10 min | Ingredients: 3 | Steps: 2\n",
      "Tags: 15-minutes-or-less, time-to-make, course, main-ingredient, preparation, for-1-or-2, lunch, side-dish...\n",
      "Description: a fun way to get kids eating a whole bunch of vegetables, without much cleaning up! i learned this recipe while i was eating mostly raw, but i wouldn'...\n",
      "\n",
      "[3] snack stackers  lunch box surprise\n",
      "Similarity Score: 0.6077\n",
      "Cooking Time: 5 min | Ingredients: 3 | Steps: 2\n",
      "Tags: 15-minutes-or-less, time-to-make, course, main-ingredient, cuisine, preparation, occasion, north-ame...\n",
      "Description: nothing fancy here.... our 5 year old loves those snackables, so i make my own rendition for his snack time at kindergarten. i like it better because ...\n",
      "\n",
      "[4] kids snack mix\n",
      "Similarity Score: 0.5941\n",
      "Cooking Time: 5 min | Ingredients: 5 | Steps: 2\n",
      "Tags: 15-minutes-or-less, time-to-make, course, preparation, 5-ingredients-or-less, appetizers, lunch, sna...\n",
      "Description: this can be bought from most stores in a small bag, but it costs so much money and is gone too soon. i decided to make it on my own with similar ingre...\n",
      "\n",
      "[5] pasta and meatballs for kids\n",
      "Similarity Score: 0.5860\n",
      "Cooking Time: 20 min | Ingredients: 9 | Steps: 9\n",
      "Tags: weeknight, 30-minutes-or-less, time-to-make, course, main-ingredient, cuisine, preparation, occasion...\n",
      "Description: this tastes just like the canned version, but with more control over ingredients you can be sure of what your child is eating. make a doubled batch an...\n"
     ]
    }
   ],
   "source": [
    "query = \"easy meal for kids\"\n",
    "\n",
    "# TF-IDF Results\n",
    "tfidf_results = tfidf_engine.search(query, top_k=5)\n",
    "display_search_results(tfidf_results, metadata_df, corpus_df, query, show_snippet=False)\n",
    "\n",
    "# Embeddings Results\n",
    "embed_results = embeddings_engine.search(query, top_k=5)\n",
    "display_search_results_embeddings(embed_results, metadata_df, corpus_df, query, engine_name=\"EMBEDDINGS\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
